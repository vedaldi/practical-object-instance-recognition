<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>VGG Practical</title>
  <link rel="stylesheet" href="base.css" />
  <link rel="stylesheet" href="prism.css" />
</head>
<body>
<h1 id="recognition-of-object-instances-practical">Recognition of object instances practical</h1>
<p>This is an <a href="http://www.robots.ox.ac.uk/~vgg">Oxford Visual Geometry Group</a> computer vision practical, authored by <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and Andrew Zisserman (Release 2018a).</p>
<p><img width=500 src="images/cover.png" alt="cover"/></p>
<p>The goal of instance-level recognition is to match (recognize) a specific object or scene. Examples include recognizing a specific building, such as Notre Dame, or a specific painting, such as ''Starry Night'' by Van Gogh. The object is recognized despite changes in scale, camera viewpoint, illumination conditions and partial occlusion. An important application is image retrieval -- starting from an image of an object of interest (the query), search through an image dataset to obtain (or retrieve) those images that contain the target object.</p>
<p>The goal of this session is to get basic practical experience with the methods that enable specific object recognition. It includes: (i) using SIFT features to obtain sparse matches between two images; (ii) using affine co-variant detectors to cover changes in viewpoint; (iii) vector quantizing the SIFT descriptors into visual words to enable large scale retrieval; and (iv) constructing and using an image retrieval system to identify objects.</p>
<div class="toc">
<ul>
<li><a href="#recognition-of-object-instances-practical">Recognition of object instances practical</a><ul>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#part-i-sparse-features-for-matching-object-instances">Part I: Sparse features for matching object instances</a><ul>
<li><a href="#stage-ia-sift-features-detector">Stage I.A: SIFT features detector</a></li>
<li><a href="#stage-ib-sift-features-descriptors-and-matching-between-images">Stage I.B: SIFT features descriptors and matching between images</a></li>
<li><a href="#stage-ic-improving-sift-matching-using-lowes-second-nearest-neighbour-test">Stage I.C: Improving SIFT matching using Lowe’s second nearest neighbour test</a></li>
<li><a href="#stage-id-improving-sift-matching-using-a-geometric-transformation">Stage I.D: Improving SIFT matching using a geometric transformation</a></li>
</ul>
</li>
<li><a href="#part2">Part II: Affine co-variant detectors</a></li>
<li><a href="#part3">Part III: Towards large scale retrieval</a><ul>
<li><a href="#stage-iiia-accelerating-descriptor-matching-with-visual-words">Stage III.A: Accelerating descriptor matching with visual words</a></li>
<li><a href="#stage3b">Stage III.B: Searching with an inverted index</a></li>
<li><a href="#stage3c">Stage III.C: Geometric rescoring</a></li>
<li><a href="#stage3d">Stage III.D: Full system</a></li>
</ul>
</li>
<li><a href="#part-iv-large-scale-retrieval">Part IV: Large scale retrieval</a></li>
<li><a href="#links-and-further-work">Links and further work</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#history">History</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="getting-started">Getting started</h2>
<p>Read and understand the <a href="../overview/index.html#installation">requirements and installation instructions</a>. The download links for this practical are:</p>
<ul>
<li>Code and data: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-instance-recognition-2018a.tar.gz">practical-instance-recognition-2018a.tar.gz</a> 560MB</li>
<li>Code only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-instance-recognition-2018a-code-only.tar.gz">practical-instance-recognition-2018a-code-only.tar.gz</a> 10MB</li>
<li>Data only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-instance-recognition-2018a-data-only.tar.gz">practical-instance-recognition-2018a-data-only.tar.gz</a> 550MB</li>
<li><a href="https://github.com/vedaldi/practical-object-instance-recognition">Git repository</a> (for lab setters and developers)</li>
</ul>
<p>After the installation is complete, open and edit the script <code>exercise1.m</code> in the MATLAB editor. The script contains commented code and a description for all steps of this exercise, relative to Part I of this document. You can cut and paste this code into the MATLAB window to run it, and will need to modify it as you go through the session. Other files <code>exercise2.m</code>, <code>exercise3.m</code>, and <code>exercise4.m</code> are given for Part II, III, and IV.</p>
<p><strong>Note</strong>: the student packages contain only the code required to run the practical. The complete package, including code to preprocess the data, is available on GitHub.</p>
<h2 id="part-i-sparse-features-for-matching-object-instances">Part I: Sparse features for matching object instances</h2>
<h3 id="stage-ia-sift-features-detector">Stage I.A: SIFT features detector</h3>
<p>The SIFT feature has both a detector and a descriptor. We will start by computing and visualizing the SIFT feature detections for two images of the same object (a building facade). Load an image, rotate and scale it, and then display the original and transformed pair:</p>
<pre><code class="language-matlab">% Load an image
im1 = imread('data/oxbuild_lite/all_souls_000002.jpg') ;
% Let the second image be a rotated and scaled version of the first
im3 = imresize(imrotate(im1,35,'bilinear'),0.7) ;% Display the images
subplot(1,2,1) ; imagesc(im1) ; axis equal off ; hold on ;
subplot(1,2,2) ; imagesc(im3) ; axis equal off ;
</code></pre>

<p>A SIFT frame is a circle with an orientation and is specified by four parameters: the center $t_x$, $t_y$, the scale $s$, and the rotation $\theta$ (in radians), resulting in a vector of four parameters $(s, \theta, t_x, t_y)$. Now compute and visualise the SIFT feature detections (frames):</p>
<pre><code class="language-matlab">% Compute SIFT features for each
[frames1, descrs1] = getFeatures(im1, 'peakThreshold', 0.01) ;
[frames3, descrs3] = getFeatures(im3, 'peakThreshold', 0.01) ;
subplot(1,2,1) ; imagesc(im1) ; axis equal off ; hold on ;
vl_plotframe(frames1, 'linewidth', 2) ;
subplot(1,2,2) ; imagesc(im3) ; axis equal off ; hold on ;
vl_plotframe(frames3, 'linewidth', 2) ;
</code></pre>

<p>Examine the second image and its rotated and scaled version and convince yourself that the detections overlap the same scene regions (even though the circles have moved their image position and changed radius). It is helpful to zoom into a smaller image area using the MATLAB magnifying glass tool. This demonstrates that the detection process transforms (is co-variant) with translations, rotations and isotropic scalings. This class of transformations is known as a similarity or equiform.</p>
<blockquote>
<p><strong>Task:</strong> The number of detected features can be controlled by changing the <code>peakThreshold</code> option. A larger value will select features that correspond to higher contrast structures in the image. Try this now: run again the same code, but increase <code>peakThreshold</code> two or three times.</p>
</blockquote>
<p>Now repeat the exercise with a pair of natural images. Start by loading the second one:</p>
<pre><code class="language-matlab">% Load a second image
im2 = imread('data/oxbuild_lite/all_souls_000015.jpg') ;
</code></pre>

<p>and plot images and feature frames. Again you should see that many of the detections overlap the same scene region. Note that, while repeatability occurs for the pair of natural views, it is much betterfor the synthetically rotated pair.</p>
<blockquote>
<p><strong>Question:</strong> Note the change in density of detections across the image. Why does it change? Will it be a problem for matching? How could it be avoided?</p>
<p><strong>Question:</strong> Occasionally, a feature is detected multiple times, with different orientations. This may happen when the orientation  assignment is ambiguous. Which kind of image structure would result in ambiguous orientation assignment?</p>
</blockquote>
<h3 id="stage-ib-sift-features-descriptors-and-matching-between-images">Stage I.B: SIFT features descriptors and matching between images</h3>
<p>Next we will use the descriptor computed over each detection to match the detections between images. We will start with the simplest matching scheme (first nearest neighbour of descriptors) and then add more sophisticated methods to eliminate any mismatches.</p>
<ul>
<li>Visualize the SIFT descriptors for the detected feature frames with the function <code>vl_plotsiftdescriptor</code>. Then use <code>vl_plotframe</code> to overlay the corresponding frames.</li>
</ul>
<blockquote>
<p><strong>Question:</strong>  Note the descriptors are computed over a much larger region (shown in blue) than the detection (shown in green). Why?</p>
</blockquote>
<ul>
<li>
<p>Compute first nearest neighbours matches - for each SIFT descriptor in the first image, compute its nearest neighbour in the second image with the function <code>findNeighbours</code>.</p>
</li>
<li>
<p>Visualize the correspondences using lines joining matched SIFT features with the function <code>plotMatches</code>.</p>
</li>
</ul>
<blockquote>
<p><strong>Question:</strong>     Notice that there are many mismatches. Examine some of the mismatches to understand why the mistakes are being made. For example, is the change in lighting a problem? What additional constraints can be applied to remove the mismatches?</p>
</blockquote>
<p><strong>Hint:</strong> You can visualize a subset of the matches using:</p>
<pre><code class="language-matlab">figure; plotMatches(im1,im2,frames1,frames2,matches(:,3:200:end));
</code></pre>

<h3 id="stage-ic-improving-sift-matching-using-lowes-second-nearest-neighbour-test">Stage I.C: Improving SIFT matching using Lowe’s second nearest neighbour test</h3>
<p>Lowe introduced a second nearest neighbour (2nd NN) test to identify, and hence remove, ambiguous matches. The idea is to identify distinctive matches by a threshold on the ratio of first to second NN distances. In the MATLAB file, the ratio is <code>nnThreshold</code> = 1NN distance / 2NN distance.</p>
<ul>
<li>Vary the ratio <code>nnThreshold</code> in a range from 0.1 to 0.9, and examine how the number of matches and number of mismatches changes.</li>
<li>A value of <code>nnThreshold = 0.8</code> is often a good compromise between losing too many matches and rejecting mismatches.</li>
</ul>
<blockquote>
<p><strong>Question:</strong> Examine some of the remaining mismatches  to understand why they have occurred. How could they be removed?</p>
</blockquote>
<h3 id="stage-id-improving-sift-matching-using-a-geometric-transformation">Stage I.D: Improving SIFT matching using a geometric transformation</h3>
<p>In addition to the 2nd NN test, we can also require consistency between the matches and a geometric transformation between the images. For the moment we will look for matches that are consistent with a similarity transformation</p>
<p>
<script type="math/tex; mode=display">
\begin{bmatrix}
x' \\ y'
\end{bmatrix}
=
sR(\theta)
\begin{bmatrix}
x \\ y
\end{bmatrix}
+
\begin{bmatrix}
t_x \\ t_y
\end{bmatrix}
</script>
</p>
<p>which consists of a rotation by $\theta$, an isotropic scaling (i.e. same in all directions) by s, and a translation by a vector $(t_x, t_y)$. This transformation is specified by four parameters $(s,\theta,t_x,t_y)$ and can be computed from a single correspondence between SIFT detections in each image.</p>
<blockquote>
<p><strong>Task:</strong> Work out how to compute this transformation from a single correspondence.</p>
</blockquote>
<p><strong>Hint:</strong> Recall from Stage I.A that a SIFT feature frame is an oriented circle and map one onto the other.</p>
<p>The matches consistent with a similarity can then be found using a RANSAC inspired algorithm, implemented by the function <code>geometricVerification</code>:</p>
<p><strong>RANSAC-like algorithm for geometric verification</strong></p>
<ol>
<li>For each tentative correspondence in turn:<ol>
<li>compute the similarity transformation;</li>
<li>map all the SIFT detections in one image to the other using this transformation;</li>
<li>accept matches that are within a threshold distance to the mapped detection (inliers);</li>
<li>count the number of accepted matches;</li>
<li>optionally, fit a more accurate affine transformation or homography to the accepted matches and test re-validate the matches.</li>
</ol>
</li>
<li>Choose the transformation with the highest count of inliers.</li>
</ol>
<hr />
<p>After this algorithm the inliers are consistent with the transformation and are retained, and most mismatches should now be removed.</p>
<blockquote>
<p><strong>Task:</strong> The figure generated by <code>plotMatches</code> supports the interactive visualisation of the transformation found by <code>geometricVerification</code>. Try hovering with the mouse on the figure and check that corresponding image points are highlighted in the two images.</p>
</blockquote>
<p><strong>Skip to <a href="#part2">Part 2</a> on fast track</strong></p>
<blockquote>
<p><strong>Task:</strong> Test this procedure by varying the threshold distance (edit <code>geometricVerification</code> and change the <code>opts.tolerance1</code>, <code>opts.tolerance2</code>, and <code>opts.tolerance3</code> parameters, where the last two thresholds are relative to the optional iterative fitting of an affine transformation or homography to the inliers). Note the number of inliers and number of mismatches.</p>
</blockquote>
<p>If more matches are required the geometric transformation can be used alone, without also requiring the 2nd NN test. Indeed, since the 1st NN may not be the correct match, a list of potential (putative) matches can be generated for each SIFT descriptor by including the 1st NN, 2nd NN, 3rd NN etc. Investigate how the number of correct matches (and time for computation) grows as the potential match list is extended, and the geometric transformation is used to select inliers. To this end:</p>
<blockquote>
<p><strong>Task:</strong> Change the code to include in the match list the 1st NN, 2nd NN, 3rd NN, … best matches for each feature.
<strong>Task:</strong> Run geometric verification and check the number of verified matches using this expanded list.</p>
</blockquote>
<p><strong>Hint:</strong> You can use MATLAB’s tic and toc functions to measure the execution time of a snippet of code. For example</p>
<pre><code class="language-matlab">tic ; pause(3) ; toc
</code></pre>

<p>will pause MATLAB for three seconds and return an elapsed time approximately equal to 3. See <code>help tic</code> for details.</p>
<h2 id="part2">Part II: Affine co-variant detectors</h2>
<p>So far the change in viewpoint between images has been a similarity transformation. Now we consider more severe viewpoint changes - for example where an object is fronto-parallel in one view, and turns away from the camera in the other as in the graffiti wall images below:</p>
<p><img src="images/aff1.jpeg" alt="aff1" width=200px"/><img src="images/aff2.jpeg" alt="aff2" width=200px"/><img src="images/aff3.jpeg" alt="aff3" width=200px/></p>
<p>In this case, there is foreshortening (anisotropic scaling) and perspective distortions between the images (as well as in-plane rotation, translation and scaling). A circle in one image cannot cover the same scene area as a circle in the other, but an ellipse can. Affine co-variant detectors are designed to find such regions.</p>
<p>In the following we will compare the number of matches using a similarity and affine co-variant detector as the viewpoint becomes progressively more extreme. The detectors are SIFT (for similarity) and SIFT+affine adaptation (for affine), while the descriptor are in both cases SIFT.</p>
<blockquote>
<p><strong>Task:</strong> Open and examine the script <code>exercise2.m</code> in the MATLAB editor. Run the script.</p>
</blockquote>
<p>Note the behaviour in the number of verified matches as the viewpoint becomes more extreme. Observe that the matches also identify the regions of the images that are in common.</p>
<blockquote>
<p><strong>Question:</strong> The transformation between the images induced by the plane is a planar homography. The detections are only affine co-variant (not as general as a planar homography). So how can descriptors computed on these detections possibly match?</p>
</blockquote>
<p><strong>Note:</strong> There are many other detector variants that could be used for this task. These can be activated by the method option of getFeatures.m (see also help <code>vl_covdet</code>).</p>
<h2 id="part3">Part III: Towards large scale retrieval</h2>
<p>In large scale retrieval the goal is to match a query image to a large database of images (for example the WWW or Wikipedia). The quality of a match is measured as the number of geometrically verified feature correspondences between the query and a database image. While the techniques discussed in Part I and II are sufficient to do this, in practice they require too much memory to store the SIFT descriptors for all the detections in all the database images. We explore next two key ideas: one to reduce the memory footprint and pre-compute descriptor matches; the other to speed up image retrieval.</p>
<blockquote>
<p><strong>Task:</strong> Open and edit the script <code>exercise3.m</code> in the MATLAB editor, and cut and paste to work through the following stages.</p>
</blockquote>
<h3 id="stage-iiia-accelerating-descriptor-matching-with-visual-words">Stage III.A: Accelerating descriptor matching with visual words</h3>
<p>Instead of matching feature descriptors directly as done in Part I and II, descriptors are usually mapped first to discrete symbols, also called visual words, by means of a clustering technique like K-Means. The descriptors that are assigned to the same visual word are considered matched. Each of the rows in the following figure illustrates image patches that are mapped to the same visual word, and are hence indistinguishable by the representation.</p>
<p><img src="images/words.jpeg" alt="words" width=400px/></p>
<p>Then, matching two sets of feature descriptors (from two images) reduces to finding the intersection of two sets of symbols.</p>
<blockquote>
<p><strong>Tasks:</strong></p>
<ul>
<li>Load a visual word dictionary and an associated approximate nearest neighbour (ANN) matcher (the ANN matcher is used to determine the closest visual word to each descriptor and is based on a forest of KD trees).</li>
<li>Given SIFT descriptors for two images, quantise them (assign them) into the corresponding visual words.</li>
<li>Find corresponding features by looking for the same visual words in the two images and note the computation time.</li>
<li>Geometrically verify these initial correspondences and count the number of inlier matches found.</li>
<li>Find corresponding features by using the method of Part I and II, i.e. by comparing the descriptors directly, and note the computation time. Geometrically verify these initial correspondences and count the number of inlier matches found.</li>
<li>Compare the speed and number of inliers when using visual words vs raw SIFT descriptors by means of the function <code>matchWords</code>. Note, you should repeat the timing (by running the matching again) as the first time you run it there may be a delay as certain MATLAB components are loaded into memory.</li>
<li><strong>Optional:</strong> compare the speed and number of matches over another pair of images (from part I and II).</li>
</ul>
<p><strong>Questions:</strong></p>
<ul>
<li>The size of the vocabulary (the number of clusters) is an important parameter in visual word algorithms. How does the size affect the number of inliers and the difficulty of computing the transformation?</li>
<li>In the above procedure the time required to convert the descriptors into visual words was not accounted for. Why?</li>
<li>What is the speedup in searching a large, fixed database of 10, 100, 1000 images?</li>
</ul>
</blockquote>
<p><strong>Skip to <a href="stage3b">Stage III.B</a> on fast track</strong> </p>
<p>Often multiple feature occurrences are mapped to the same visual word. In this case <code>matchWords</code> generates only one of the possible matches.</p>
<blockquote>
<p><strong>Tasks:</strong></p>
<ul>
<li>Modify <code>matchWords</code> to generate more than one match for cases in which multiple features are mapped to the same visual word.This can be achieved by increasing the value of <code>maxNumMatches</code>.</li>
<li>Most of these additional matches are incorrect. Filter them out by running <code>geometricVerification</code>.</li>
<li>Compare the number of inliers obtained before and after this modification.</li>
</ul>
</blockquote>
<h3 id="stage3b">Stage III.B: Searching with an inverted index</h3>
<p>While matching with visual words is much faster than doing so by comparing feature descriptors directly, scoring images directly based on the number of geometrically verified matches still entails fitting a geometric model, a relatively slow operation. Rather than scoring all the images in the database in this way, we are going to use an approximation and count the number of visual words shared between two images.</p>
<p>To this end, one computes a histogram of the visual words in a query image and for each of  the database images. Then the number of visual words in common can be computed from the intersection of the two histograms.</p>
<p>The histogram intersection can be thought as a similarity measure between two histograms. In practice, this measure can be refined in several ways:</p>
<ul>
<li>By reducing the importance of common visual words. This is similar to a stop-words list and can be implemented by weighting each word by the `inverse document frequency' (the inverse of the frequency of occurrence of that visual word over the entire database of images).</li>
<li>By normalising the weighted histograms to unit vectors and using the cosine between them as similarity. This can be implemented easily as the inner product between normalised histograms.</li>
</ul>
<p>Computing histogram similarities can be implemented extremely efficiently using an inverted file index. In this exercise, inner products between normalized histograms are computed quite efficiently using MATLAB's built-in sparse matrix engine.</p>
<p>We now apply this retrieval method to search using a query image within a 660 image subset of the Oxford 5k building image set.</p>
<blockquote>
<p><strong>Task:</strong> How many erroneously matched images do you count in the top results?
<strong>Question:</strong> Why does the top image have a score of 1?</p>
</blockquote>
<h3 id="stage3c">Stage III.C: Geometric rescoring</h3>
<p>Histogram-based retrieval results are good but far from perfect. Given a short list of top ranked images from the previous step, we are now going to re-score them based on the number of inlier matches after a geometric verification step.</p>
<blockquote>
<p><strong>Question:</strong> Why is the top score much larger than 1 now?
<strong>Question:</strong> Are the retrieval results improved after geometric verification?</p>
</blockquote>
<h3 id="stage3d">Stage III.D: Full system</h3>
<p>Now try the full system to retrieve matches to an unseen query image.</p>
<p><img src="images/query.jpeg" alt="query" width-=400px/></p>
<h2 id="part-iv-large-scale-retrieval">Part IV: Large scale retrieval</h2>
<p><strong>Skip and end here on fast track</strong></p>
<p>The images below are all details of paintings. The goal of this last part of the practical is to identify the paintings that they came from. For this we selected a set of 1734 images of paintings from Wikipedia.</p>
<p><img src="images/paint1.jpeg" alt="paint1" height=200px/><img src="images/paint2.jpeg" alt="paint2" height=200px/><img src="images/paint3.jpeg" alt="paint3" height=200px/></p>
<p>To identify the details you can either:</p>
<ol>
<li>use your knowledge of art</li>
<li>search through the 1734 Wikipedia images until you find matches</li>
<li>build a recognition system and match the details automatically</li>
</ol>
<p>We follow route (3) here. Look through and run <code>exercise4.m</code>. This uses the techniques described in Part III in order to construct an index for 1734 Wikipedia images so that they may be searched quickly. Use the code to find from which paintings these details come from.</p>
<p>Note, although the index is stored locally, the matching images are downloaded from Wikipedia and displayed. Click on the image to reach the Wikipedia page for that painting (and hence identify it).</p>
<blockquote>
<p><strong>Task:</strong> Use the code to visually search Wikipedia for further paintings from Van Gogh downloaded from the Internet. </p>
</blockquote>
<p><strong>Note:</strong> the code supports URL in place of filenames. </p>
<p>Take note of the code output.</p>
<blockquote>
<p><strong>Questions:</strong></p>
<ul>
<li>How many features are there in the painting database?</li>
<li>How much memory does the image database take?</li>
<li>What are the stages of the search? And how long does each of the stages take for one of the query images?</li>
</ul>
</blockquote>
<p>That completes this practical.</p>
<h2 id="links-and-further-work">Links and further work</h2>
<ul>
<li>The code for this practical is written using the software package <a href="http://www.vlfeat.org">VLFeat</a>. This is a software library written in MATLAB and C, and is freely available as source code and binary.</li>
<li>The images for this practical are taken from the <a href="http://www.robots.ox.ac.uk/~vgg/research/affine/">Affine Covariant Features dataset</a>, and the <a href="http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/">Oxford Buildings benchmark</a>.</li>
<li>For a tutorial on large scale visual search and references to the literature, see the lectures by Josef Sivic and Cordelia Schmid <a href="http://www.di.ens.fr/willow/events/cvml2012/materials/">here</a>.</li>
<li>For recent developments in large scale search (compact image descriptors, compression with product quantization), see <a href="https://sites.google.com/site/lsvr13/">these lectures</a> by Herve Jegou and Florent Perronnin.</li>
</ul>
<h2 id="acknowledgements">Acknowledgements</h2>
<ul>
<li>Guidance from Josef Sivic, Ivan Laptev and Cordelia Schmid</li>
<li>Mircea Cimpoi for scripts for downloading and linking to Wikipedia paintings </li>
<li>Comments from Relja Arandjelovic, Karen Simonyan, Omkar Parkhi, Meelis Lootus, Hossein Azizpour, Max Jaderberg</li>
<li>Funding from ERC grant VisRec Grant No. 228180, and a PASCAL Harvest Grant.</li>
</ul>
<p><img src="images/erc.jpeg" alt="erc" height=100px/><img src="images/PASCAL2.png" alt="pascal2" height=100px/></p>
<h2 id="history">History</h2>
<ul>
<li>Used in the Oxford AIMS CDT, 2014-18</li>
<li>Used at <a href="http://www.di.ens.fr/willow/events/cvml2012/">ENS/INRIA Visual Recognition and Machine Learning Summer School, 2012</a>.</li>
<li>Used at <a href="http://www.clsp.jhu.edu/workshops/archive/ws-12/summer-school/">JHU Summer School on Human Language Technology, 2012</a>.</li>
</ul><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
  "HTML-CSS": { availableFonts: ["TeX"] },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
if (typeof MathJaxListener !== 'undefined') {
  MathJax.Hub.Register.StartupHook('End', function () {
    MathJaxListener.invokeCallbackForKey_('End');
  });
}
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="prism.js"></script>
</body>
</html>
